{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHYS 321 Coding Lab #7: Markov Chain Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this coding lab, we apply the technique of Markov Chain Monte Carlo (MCMC) sampling to sample several posterior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a copy of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, don't forget to make a copy of this notebook with your initials appended, and to work in your new copy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the ``corner`` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need the ``corner`` Python package to help us make pretty triangle/corner plots. Try running the command ``import corner``. If you get an error, fire up your terminal and execute ``conda install corner``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redoing the Maxwell-Boltzmann problem with MCMCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a warm-up, we will redo the Maxwell-Boltzmann  problem from the Coding Midterm. Reducing the problem to its mathematical barebones, recall that we have measured a set of velocities $v=(20, 30, 28.5, 18, 15, 20, 20)$ that are drawn from Maxwell-Boltzmann distributions (with funny units):\n",
    "\n",
    "\\begin{equation}\n",
    "p(v_i|T) = 4 \\pi \\left(\\frac{1}{2 \\pi  T} \\right)^{3/2}  v_i^2 \\exp \\left( \\frac{-v_i^2}{2 T} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "We don't know the temperature $T$, and we would like to infer it from the data. Bayes' theorem tells us\n",
    "\n",
    "\\begin{equation}\n",
    "p(T | v_1, v_2, \\dots) \\propto p(v_1, v_2, \\dots|T ) p (T).\n",
    "\\end{equation}\n",
    "If we assume that the measurements are independent, then the likelihood can be written as\n",
    "\\begin{equation}\n",
    "p(v_1, v_2, \\dots|T ) = p(v_1 |T) p(v_2|T) \\dots\n",
    "\\end{equation}\n",
    "which means our posterior is now\n",
    "\\begin{equation}\n",
    "p(T | v_1, v_2, \\dots) \\propto p (T) \\prod_i \\left[ \\left(\\frac{1}{2 \\pi  T} \\right)^{3/2}  v_i^2 \\exp \\left( \\frac{-v_i^2}{2 T} \\right) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "Adopting a uniform prior between $T=1$ and $T=2000$, one obtains the following posterior distribution for $T$:\n",
    "<div>\n",
    "<img src=\"images/MaxwellBoltzmannPosterior.png\" width=\"400\">\n",
    "</div>\n",
    "With just one parameter (and therefore a one-dimensional parameter space), it is computationally feasible to just plot the posterior by brute force evaluation. But as a warmup for the other exercises in this Coding Lab, let's also solve this problem using MCMCs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the Metropolis algorithm for MCMC sampling goes as follows:\n",
    "1. Starting at some point $\\boldsymbol{\\theta}^{(n)}$ in parameter space...\n",
    "2. Propose a step to a new location by drawing randomly from the proposal distribution $q(\\boldsymbol{\\theta}^\\prime | \\boldsymbol{\\theta}^{(n)})$, e.g., $q(\\boldsymbol{\\theta}^\\prime | \\boldsymbol{\\theta}^{(n)}) \\propto \\exp\\left[ - \\frac{|\\boldsymbol{\\theta}^\\prime - \\boldsymbol{\\theta}^{(n)}|^2}{2 \\eta^2}\\right]$\n",
    "3. Draw a random number $r$ from a uniform distribution over $0<r<1$.\n",
    "4. If $\\frac{p(\\boldsymbol{\\theta}^\\prime | \\textrm{data})}{p(\\boldsymbol{\\theta}^{(n)} | \\textrm{data})} > r$, then accept the step and call $\\boldsymbol{\\theta}^{(n+1)} = \\boldsymbol{\\theta}^\\prime\n",
    "$. If not, then repeat the last location in the chain and set $\\boldsymbol{\\theta}^{(n+1)} = \\boldsymbol{\\theta}^{(n)}$\n",
    "5. Go back to Step 1 and keep iterating!\n",
    "\n",
    "This will give a chain of values that are drawn from the probability distribution. Plotting those values in a histogram then gives an (excellent) approximation to the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__: Implement a Metroplis algorithm and use it to perform an MCMC sampling of the posterior $p(T | v)$. Plot your result. Feel free to refer to my Search & Rescue with MCMCs notebook available on myCourses. (Hint: the longer you run your MCMC algorithm, the prettier your plots will be. But obviously you don't want to wait forever. I find that running for 50000 steps gives me a reasonable plot. This takes a few minutes on my laptop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a pre-written package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's not hard to code up the most basic MCMC algorithms (you just did it!), we often rely on standard packages that have nice implementations of more advanced MCMC algorithms that employ various clever tricks. Today we'll learn to use the ``emcee`` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll illustrate this by tackling the simple example of fitting a straight line. (This next example is adapted from this [excellent tutorial](https://emcee.readthedocs.io/en/stable/tutorials/line/)). Here's some pre-generated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('straight_line.dat')\n",
    "x_vals, y_vals = data\n",
    "errs = np.ones_like(y_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data were generated with error bars of size $\\sigma = 1$, so they look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(6,4))\n",
    "ax.errorbar(x_vals, y_vals, yerr=errs, linestyle = 'None',capsize=4, marker ='o', color='black', ms=8)\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A straight-line model for the data takes the form $y^\\mathcal{M}(a, b, x) = a x + b$, so we have a two-parameter model. Our goal is to study the posterior distribution of $a$ and $b$:\n",
    "\n",
    "\\begin{equation}\n",
    "p(a, b | \\{ y_i \\}) \\propto p(\\{ y_i \\} | a, b) p(a,b).\n",
    "\\end{equation}\n",
    "\n",
    "If we assume that the errors are Gaussian, the likelihood is given by\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\{ y_i \\} | a, b) =  \\prod_{i=1}^{13} \\frac{\\exp\\left[-\\frac{\\left[y_i - a x_i - b\\right]^2}{2 \\sigma_i^2}\\right]}{\\sqrt{2 \\pi \\sigma_i^2}}  \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably found in Coding Lab 07, probability density functions can have incredibly large dynamic ranges (i.e., the height of the highest peak is very large compared height of the lowest point). For this reason, it is often advisable to work with the *logarithm* of probability distribution functions. This is what we must provide to ``emcee``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(theta, x, y, yerr):\n",
    "    a, b = theta\n",
    "    model = a * x + b\n",
    "    sigma2 = yerr ** 2\n",
    "    return -0.5 * np.sum((y - model) ** 2 / sigma2 + np.log(sigma2)) # the 2pi factor doesn't affect the shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to provide a prior. We'll use a uniform prior between $-5$ and $+5$ for the intercept $b$, and a uniform prior between $-1$ and $5$ for the slope $a$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(theta):\n",
    "    a, b = theta\n",
    "    if -1.0 < a < 5. and -5. < b < 5.:\n",
    "        return 0.0 # the constant doesn't matter since MCMCs only care about *ratios* of probabilities\n",
    "    return -np.inf # log(0) = -inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, the posterior is proportional to the likelihood multiplied by the prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_post(theta, x, y, yerr):\n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta, x, y, yerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we unleash ``emcee`` and let it perform an MCMC for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 5000\n",
    "ndim = 2 # number of parameters\n",
    "nwalkers = 32\n",
    "initial_pos = np.array((2.6, 1.3)) + 0.01 * np.random.randn(nwalkers, ndim)\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_post, args=(x_vals, y_vals, errs))\n",
    "sampler.run_mcmc(initial_pos, num_iter, progress=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack the above commands a little. The variable ``num_iter`` tells ``emcee`` how many MCMC iterations to go through, while ``ndim`` is the number of dimensions of our parameter space (here it's $2$ because we have a slope and an intercept to solve for). The ``EnsembleSampler`` is a type of MCMC that doesn't use just one walker, but instead, has a whole *ensemble* of walkers that crawl over the posterior together to \"feel\" its shape. Here we're using $32$ walkers. I picked the initial walker positions to be close to $(a, b) = (2.6, 1.3)$, with each walker deviating from that position by some small perturbation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the MCMC chains, we type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.get_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting these samples gives us our trace plot, with each colour corresponding to a different walker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, figsize=(10, 7), sharex=True)\n",
    "samples = sampler.get_chain()\n",
    "labels = [\"a\", \"b\"]\n",
    "for i in range(ndim):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples[:, :, i], alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(labels[i])\n",
    "    #ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "axes[-1].set_xlabel(\"Step number\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the chains from all the walkers into one chain while 1) discarding the first 100 steps as burn-in steps, and 2) thinning the chain by taking only one out of every 15 steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_samples = sampler.get_chain(discard=100, thin=15, flat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make our corner/triangle plot to visualize our results. We've marked the median and the 16th/84th percentiles so that you can see what the 68% credibility region looks like for each parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = corner.corner(flat_samples, labels=labels, quantiles=[0.16, 0.5, 0.84]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also helpful to take a random selection of parameter values in our chain and to plot what they look like against our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.random.randint(len(flat_samples), size=100)\n",
    "x0 = np.linspace(0, 10., 13)\n",
    "f, ax = plt.subplots(figsize=(6,4))\n",
    "for ind in inds:\n",
    "    sample = flat_samples[ind]\n",
    "    ax.plot(x0, sample[0] * x0 + sample[1], alpha=0.05, color='red')\n",
    "ax.errorbar(x_vals, y_vals, yerr=errs, linestyle = 'None',capsize=4, marker ='o', color='black', ms=8)\n",
    "ax.set_xlim(0, 10.)\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial transparency of our best-fit lines allows us to easily see where the prediction is very tight (intense colours) and where the prediction is quite uncertain (faint colours)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The next set of exercises are adapted from the [excellent blog post](http://jakevdp.github.io/blog/2014/06/06/frequentism-and-bayesianism-2-when-results-differ/) by Jake VanderPlas). Now suppose we had a dataset like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = np.loadtxt('straight_line_outlier.dat')\n",
    "x2_vals, y2_vals = data2\n",
    "errs2 = np.ones_like(y2_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(6,4))\n",
    "ax.errorbar(x2_vals, y2_vals, yerr=errs2, linestyle = 'None',capsize=4, marker ='o', color='black', ms=8)\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__: Fit this data and plot your results. You may use an optimal linear estimator, a direct evaluation of a posterior distribution, or an MCMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that there are three points here that are outliers that have skewed your data so that the slope is shallower than you might've expected. \"Normally\", one might just be tempted to throw out these data points. But this seems a little arbitrary. What if they had been a little closer to the other points? At what point do we declare that they are \"far enough\" to be thrown out as outliers? It turns out that a Bayesian approach allows the data to decide which datapoints are outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is that just from a quick glance of the data, it's clear that a linear model doesn't fit the data. We should come up with a more general model that includes the possibility of outliers. One possible model is one where the likelihood is given by\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\{ y_i \\} | a, b, \\{g_i\\}, \\gamma) =  \\prod_{i=1}^{16} \\left( \\frac{g_i}{\\sqrt{2 \\pi \\sigma_i^2}} \\exp\\left[-\\frac{\\left(y_i - a x_i - b\\right)^2}{2 \\sigma_i^2}\\right]  + \\frac{(1-g_i)}{\\sqrt{2 \\pi \\gamma^2}} \\exp\\left[-\\frac{\\left(y_i - a x_i - b\\right)^2}{2 \\gamma^2}\\right]\\right)\n",
    "\\end{equation}\n",
    "\n",
    "We now have a lot of parameters! In addition to $a$ and $b$, we now have a $g_i$ parameter associated with each data point. There's also a new parameter that we have called $\\gamma$. For simplicity in this exercise we will not treat it as a free parameter, but instead we will set it to some large value $\\gamma = 25$. But even with $\\gamma$ fixed, we have gone from a 2-dimensional parameter space to a 18-dimensional parameter space!\n",
    "\n",
    "The model that we have written down is one where each data point has some probability $g_i$ of being drawn from Gaussian we used before; if it isn't drawn from this distribution, it's instead drawn from a different Gaussian distribution that has standard deviation $\\gamma$. Since $\\gamma \\gg \\sigma_i$, this is a very broad distribution that accounts for the possibility that the data might be drawn from an \"outlier distribution\".\n",
    "\n",
    "As usual, Bayes' theorem tells us that\n",
    "\\begin{equation}\n",
    "p( a, b, \\{g_i\\}, \\gamma | \\{ y_i \\} ) \\propto p(\\{ y_i \\} | a, b, \\{g_i\\}, \\gamma) p(a, b, \\{g_i\\}, \\gamma)\n",
    "\\end{equation}\n",
    "\n",
    "Once we compute the posterior distribution, we can marginalize over all the parameters except for a particular $g_i$. The resulting distribution for $g_i$ can then tell us whether we should treat a data point as an outlier or not. If $g_i$ is peaked near $1$, then the data point is probably not an outlier. If it's peaked near $0$, it's probably an outlier. In some ways, though, this is really just something that we do to satisfy our curiosity. A more direct way to fit the data is to simply marginalize over $\\{ g_i \\}$---which are **nuisance parameters**---leaving us with a two-dimensional posterior distribution over $a$ and $b$ like we had before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__: Do what I just suggested! With so many parameters, a direct plotting of the posterior is not practical; perform an MCMC sampling instead. Confirm that the three data points we intuitively thought were outliers are, in fact, outliers. Plot some representative fits to the data, showing that our new results are no longer so skewed by the outliers. (Your MCMC may take a few minutes to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to SNe Ia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that way back in Coding Lab 02, we looked at how SNe Ia could be used as standard candles to constrain cosmological parameters. Let's look at that data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_z,sn_dm,sn_dm_err = np.loadtxt(\"SCPUnion2.1_mu_vs_z.txt\",delimiter=\"\\t\",\\\n",
    "                                  skiprows=5, usecols = (1,2,3),unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(6,4))\n",
    "#ax[0].plot(sn_z, sn_dm, '.', color=\"black\", label=\"Type Ia SNe data\",alpha=0.25)\n",
    "ax.errorbar(sn_z, sn_dm, yerr=sn_dm_err, fmt='.k', ecolor='gray')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel(r'$m-M$ (mag)')\n",
    "ax.set_xlabel(r'$z$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from Coding Lab 02 that these data can be used to fit for $\\Omega_m$ (the normalized matter density) and $\\Omega_\\Lambda$ (the normalized dark energy density). We didn't actually do the fit during Coding Lab 02 because I wanted to wait until we could do it in a statistically disciplined way. Which is now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__: Assuming Gaussian error bars, find the posterior distribution for $\\Omega_m$ and $\\Omega_\\Lambda$. (Hint: the ``astropy`` package allows you to create ``LambdaCDM`` objects. Once these objects are supplied with a value of $H_0$, $\\Omega_\\Lambda$, and $\\Omega_m$, the ``distmod`` function contained in the object can return the distance modulus $m-M$. Here we are only interested in $\\Omega_m$ and $\\Omega_\\Lambda$, so what must you do with $H_0$?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare this to Figure 6 of the seminal [Reiss et al. (1998)](https://iopscience.iop.org/article/10.1086/300499/meta) paper, you will see that the data you're using (compiled in 2011) is more constraining. The data have only improved since then. Do note, though, that the default contours drawn by the ``corner`` package do __*not*__ represent the $68\\%$ and $95\\%$ credibility regions. [Here's](https://corner.readthedocs.io/en/latest/pages/sigmas.html) a description of what they're plotting. I actually disagree with their philsophy of what should be the default, but there's no problem as long as we're clear about what's being plotted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources for further study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, we've only barely scratched the surface when it comes to MCMCs. But you have the basics now, enough for you to use MCMCs as a workhorse tool in your data analyses! For those who want to go a little deeper, here are some nice resources:\n",
    "* [Hogg & Foreman-Mackey (2017)](https://arxiv.org/abs/1710.06068) have a really nice pedagogical treatment of MCMCs. It's written at about the same level as this class, but discusses some details that we didn't have time to cover.\n",
    "* [Geyer (2011)](http://www.mcmchandbook.net/HandbookChapter1.pdf) does things a little bit more systematically, with some slightly more detailed derivations.\n",
    "* [Betancourt (2019)](https://betanalpha.github.io/assets/case_studies/markov_chain_monte_carlo.html). Those who enjoy mathematical rigour but still appreciate helpful graphics for visualization will enjoy this treatment of MCMCs.\n",
    "* [MacKay's Information Theory, Inference, and Learning Algorithms textbook](https://www.inference.org.uk/itprnn/book.pdf) has some delightful prose. It also explores some MCMC-alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting your work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to push a copy of your completed notebook to your Github repo for marking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
